{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 7, column: 20, offset: 265} for query: '\\n            MATCH (d:Disease)\\n            OPTIONAL MATCH (d)<-[:INDICATES]-(s:Symptom)\\n            OPTIONAL MATCH (d)-[:TREATED_BY]->(sp:Specialist)\\n            OPTIONAL MATCH (d)-[:AFFECTS]->(bp:BodyPart)\\n            RETURN d.name as disease, \\n                   id(d) as node_id,\\n                   count(DISTINCT s) as symptom_count,\\n                   count(DISTINCT sp) as specialist_count,\\n                   count(DISTINCT bp) as bodypart_count,\\n                   labels(d)[0] as label\\n            '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n            MATCH (d:Disease)\\n            OPTIONAL MATCH (d)<-[:INDICATES]-(s:Symptom)\\n            OPTIONAL MATCH (d)-[:TREATED_BY]->(sp:Specialist)\\n            OPTIONAL MATCH (d)-[:AFFECTS]->(bp:BodyPart)\\n            RETURN d.name as disease, \\n                   id(d) as node_id,\\n                   count(DISTINCT s) as symptom_count,\\n                   count(DISTINCT sp) as specialist_count,\\n                   count(DISTINCT bp) as bodypart_count,\\n                   labels(d)[0] as label\\n            '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully!\n",
      "Number of diseases processed: 819\n",
      "Example embedding for first disease: [0.41312364 1.39955623]\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "class GraphEmbedder:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def extract_graph_features(self):\n",
    "        \"\"\"Extract features for each disease based on its connections\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all diseases with their connected entities\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (d:Disease)\n",
    "            OPTIONAL MATCH (d)<-[:INDICATES]-(s:Symptom)\n",
    "            OPTIONAL MATCH (d)-[:TREATED_BY]->(sp:Specialist)\n",
    "            OPTIONAL MATCH (d)-[:AFFECTS]->(bp:BodyPart)\n",
    "            RETURN d.name as disease, \n",
    "                   id(d) as node_id,\n",
    "                   count(DISTINCT s) as symptom_count,\n",
    "                   count(DISTINCT sp) as specialist_count,\n",
    "                   count(DISTINCT bp) as bodypart_count,\n",
    "                   labels(d)[0] as label\n",
    "            \"\"\")\n",
    "            \n",
    "            features = [dict(record) for record in result]\n",
    "            return pd.DataFrame(features)\n",
    "    \n",
    "    def generate_embeddings(self, n_components=2):\n",
    "        \"\"\"Generate embeddings using PCA on the extracted features\"\"\"\n",
    "        features_df = self.extract_graph_features()\n",
    "        \n",
    "        # Select and scale numerical features\n",
    "        numerical_cols = ['symptom_count', 'specialist_count', 'bodypart_count']\n",
    "        X = features_df[numerical_cols].values\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Apply PCA - using min(n_samples, n_features) components\n",
    "        n_components = min(n_components, X_scaled.shape[1])\n",
    "        pca = PCA(n_components=n_components)\n",
    "        embeddings = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Create embedding dictionary\n",
    "        embedding_dict = {}\n",
    "        for idx, row in features_df.iterrows():\n",
    "            embedding_dict[row['node_id']] = embeddings[idx]\n",
    "            embedding_dict[row['disease']] = embeddings[idx]\n",
    "        \n",
    "        # Save additional information for reference\n",
    "        embedding_info = {\n",
    "            'embeddings': embedding_dict,\n",
    "            'feature_means': scaler.mean_,\n",
    "            'feature_stds': scaler.scale_,\n",
    "            'pca_components': pca.components_,\n",
    "            'disease_names': features_df['disease'].tolist()\n",
    "        }\n",
    "        \n",
    "        return embedding_info\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    embedder = GraphEmbedder(\"bolt://localhost:7687\", \"neo4j\", \"12345678\")\n",
    "    embedding_info = embedder.generate_embeddings(n_components=2)\n",
    "    \n",
    "    # Save embeddings\n",
    "    with open('disease_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(embedding_info, f)\n",
    "    \n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    print(f\"Number of diseases processed: {len(embedding_info['disease_names'])}\")\n",
    "    print(f\"Example embedding for first disease: {embedding_info['embeddings'][embedding_info['disease_names'][0]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    embedder.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\indir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8080\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\indir\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from neo4j import GraphDatabase\n",
    "from fuzzywuzzy import process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class MedicalSystem:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all medical data in one pass\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Load symptoms\n",
    "            result = session.run(\"MATCH (s:Symptom) RETURN s.name as name\")\n",
    "            self.all_symptoms = [record['name'] for record in result]\n",
    "            self.symptom_lower_map = {s.lower(): s for s in self.all_symptoms}\n",
    "            \n",
    "            # Load disease-symptom relationships\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (d:Disease)<-[:INDICATES]-(s:Symptom)\n",
    "            RETURN d.name as disease, collect(s.name) as symptoms\n",
    "            \"\"\")\n",
    "            self.disease_symptoms = {record['disease']: record['symptoms'] for record in result}\n",
    "            \n",
    "            # Load disease-specialist relationships\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (d:Disease)-[:TREATED_BY]->(s:Specialist)\n",
    "            RETURN d.name as disease, collect(s.name) as specialists\n",
    "            \"\"\")\n",
    "            self.disease_specialists = {record['disease']: record['specialists'] for record in result}\n",
    "            \n",
    "        # Prepare TF-IDF vectors\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self._tokenize_medical)\n",
    "        symptom_texts = [\" \".join(symptoms) for symptoms in self.disease_symptoms.values()]\n",
    "        self.disease_names = list(self.disease_symptoms.keys())\n",
    "        self.symptom_vectors = self.vectorizer.fit_transform(symptom_texts)\n",
    "\n",
    "    def _tokenize_medical(self, text: str) -> List[str]:\n",
    "        return re.findall(r\"[a-zA-Z0-9]+(?:[-'][a-zA-Z0-9]+)*\", text.lower())\n",
    "\n",
    "    def analyze(self, text: str) -> Tuple[List[Tuple[str, float]], List[Dict]]:\n",
    "        \"\"\"Return both extracted symptoms and recommendations\"\"\"\n",
    "        # 1. Extract high-confidence symptoms with scores\n",
    "        input_clean = text.lower()\n",
    "        symptoms_with_scores = []\n",
    "        \n",
    "        # Exact matching (100% confidence)\n",
    "        for symptom_lower, symptom in self.symptom_lower_map.items():\n",
    "            if symptom_lower in input_clean:\n",
    "                symptoms_with_scores.append((symptom, 100.0))\n",
    "        \n",
    "        # Fuzzy matching\n",
    "        for symptom in self.all_symptoms:\n",
    "            if symptom in [s for s, _ in symptoms_with_scores]:\n",
    "                continue\n",
    "            matches = process.extract(symptom.lower(), [input_clean], limit=1)\n",
    "            if matches and matches[0][1] > 95:\n",
    "                symptoms_with_scores.append((symptom, matches[0][1]))\n",
    "        \n",
    "        # TF-IDF matching\n",
    "        input_terms = \" \".join(self._tokenize_medical(input_clean))\n",
    "        input_vec = self.vectorizer.transform([input_terms])\n",
    "        cosine_scores = cosine_similarity(input_vec, self.symptom_vectors)[0]\n",
    "        for idx, score in enumerate(cosine_scores):\n",
    "            if score > 0.95:\n",
    "                disease = self.disease_names[idx]\n",
    "                for symptom in self.disease_symptoms[disease]:\n",
    "                    if symptom not in [s for s, _ in symptoms_with_scores]:\n",
    "                        symptoms_with_scores.append((symptom, score * 100))\n",
    "        \n",
    "        # Get just the symptom names for recommendations\n",
    "        symptom_names = [s for s, _ in symptoms_with_scores]\n",
    "        \n",
    "        # 2. Get recommendations\n",
    "        input_vec = self.vectorizer.transform([\" \".join(symptom_names)])\n",
    "        similarities = cosine_similarity(input_vec, self.symptom_vectors)[0]\n",
    "        top_indices = np.argsort(similarities)[-5:][::-1]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            disease = self.disease_names[idx]\n",
    "            recommendations.append({\n",
    "                'disease': disease,\n",
    "                'confidence': float(similarities[idx]),\n",
    "                'matching_symptoms': [s for s in symptom_names if s in self.disease_symptoms[disease]],\n",
    "                'specialists': self.disease_specialists.get(disease, [\"General Practitioner\"])\n",
    "            })\n",
    "        \n",
    "        return symptoms_with_scores, recommendations\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "# Initialize the system\n",
    "medical_system = MedicalSystem(\"bolt://44.201.21.92\", \"neo4j\", \"partitions-slave-diagrams\")\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    symptoms = []\n",
    "    results = []\n",
    "    if request.method == 'POST':\n",
    "        text = request.form.get('symptoms', '').strip()\n",
    "        if text:\n",
    "            symptoms, results = medical_system.analyze(text)\n",
    "    return render_template('index.html', symptoms=symptoms, results=results)\n",
    "\n",
    "@app.teardown_appcontext\n",
    "def shutdown(exception=None):\n",
    "    medical_system.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
